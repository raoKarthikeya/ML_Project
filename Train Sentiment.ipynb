{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current path of directory\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @GregAbbott_TX: @TedCruz: \"On my first day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @warriorwoman91: I liked her and was happy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Going on #MSNBC Live with @ThomasARoberts arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Deer in the headlights RT @lizzwinstead: Ben C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @NancyOsborne180: Last night's debate prove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@JGreenDC @realDonaldTrump In all fairness #Bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @WayneDupreeShow: Just woke up to tweet thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Me reading my family's comments about how grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @ArcticFox2016: RT @AllenWestRepub \"Dear @J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @pattonoswalt: I loved Scott Walker as Mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Hey @ChrisChristie exploiting the tragedy of 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @CarolCNN: #DonaldTrump under fire for comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @johncardillo: Guess who had most speaking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Negative</td>\n",
       "      <td>reason comment is funny 'in case you're ignora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @PamelaGeller: Huckabee: Paying for transge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ChuckNellis: Cruz has class &amp;amp; truth, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @mchamric: RT ÛÏ@TeaTraitors: #GOPDebate w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @erinmallorylong: No *I* hate Planned Paren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @thekevinryder: #GOPDebate (Vine by @dabull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @MrPooni: Fox News trying to convince us yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Negative</td>\n",
       "      <td>#GOPDebate rankings: worst to be performance -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @TheBaxterBean: Scott Walker's Abortion Ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @feministabulous: It's not a competition, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @mch7576: RT ÛÏ@TeaTraitors: #GOPDebate wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @HillaryClinton: Watch the #GOPdebate? Bet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13841</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @EthanObama: Did Trump just admit he gives ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13842</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Trump and most of the World! Just look at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13843</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@JessicaValenti they all need coif reform. #GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13844</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Wow, Rubio's ear-to-head ratio is all messed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13845</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @the818: Let's play \"how fast can we distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13846</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Me watching the GOP Debates. #gopdebates https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13847</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @mjtbaum: GOD is making an appearance at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @PuestoLoco: .@NewDay Cancel the primaries....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @kaylasmith4791: Really enjoyed everything ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: The candidates don't have to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td>Negative</td>\n",
       "      <td>#GOPDebate in a nutshell via Vote True Blue 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @erinmallorylong: No *I* hate Planned Paren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @BrendanKKirby: If @JohnKasich hasn't wrapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13854</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @monaeltahawy: I'll tell you the one good t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13855</th>\n",
       "      <td>Negative</td>\n",
       "      <td>while pro-life nonsense @ the #GOPDebate was n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13856</th>\n",
       "      <td>Negative</td>\n",
       "      <td>GOP respects life....just not black ones. #GOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13857</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This is why I don't watch Fox News, they're al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13858</th>\n",
       "      <td>Positive</td>\n",
       "      <td>@marcorubio came out of the gate like a true l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13859</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Best line of #GOPDebate was \"Immigration witho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13860</th>\n",
       "      <td>Negative</td>\n",
       "      <td>People who say they are #prolife are usually a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13861</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Why should @realDonaldTrump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13862</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RWSurferGirl: Trump has got it right, nobo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13863</th>\n",
       "      <td>Negative</td>\n",
       "      <td>So trans soldiers can die for you Huckabee but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13864</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Is it just me or does anyone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13865</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Fox is cherry picking the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13866</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @cappy_yarbrough: Love to see men who will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13867</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @georgehenryw: Who thought Huckabee exceede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13868</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @Lrihendry: #TedCruz As President, I will a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13869</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @JRehling: #GOPDebate Donald Trump says tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @Lrihendry: #TedCruz headed into the Presid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13871 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "0       Neutral  RT @NancyLeeGrahn: How did everyone feel about...\n",
       "1      Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...\n",
       "2       Neutral  RT @TJMShow: No mention of Tamir Rice and the ...\n",
       "3      Positive  RT @RobGeorge: That Carly Fiorina is trending ...\n",
       "4      Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...\n",
       "5      Positive  RT @GregAbbott_TX: @TedCruz: \"On my first day ...\n",
       "6      Negative  RT @warriorwoman91: I liked her and was happy ...\n",
       "7       Neutral  Going on #MSNBC Live with @ThomasARoberts arou...\n",
       "8      Negative  Deer in the headlights RT @lizzwinstead: Ben C...\n",
       "9      Negative  RT @NancyOsborne180: Last night's debate prove...\n",
       "10     Negative  @JGreenDC @realDonaldTrump In all fairness #Bi...\n",
       "11     Positive  RT @WayneDupreeShow: Just woke up to tweet thi...\n",
       "12     Negative  Me reading my family's comments about how grea...\n",
       "13      Neutral  RT @ArcticFox2016: RT @AllenWestRepub \"Dear @J...\n",
       "14     Positive  RT @pattonoswalt: I loved Scott Walker as Mark...\n",
       "15     Negative  Hey @ChrisChristie exploiting the tragedy of 9...\n",
       "16     Negative  RT @CarolCNN: #DonaldTrump under fire for comm...\n",
       "17     Negative  RT @johncardillo: Guess who had most speaking ...\n",
       "18     Negative  reason comment is funny 'in case you're ignora...\n",
       "19     Negative  RT @PamelaGeller: Huckabee: Paying for transge...\n",
       "20     Positive  RT @ChuckNellis: Cruz has class &amp; truth, t...\n",
       "21     Negative  RT @mchamric: RT ÛÏ@TeaTraitors: #GOPDebate w...\n",
       "22     Negative  RT @erinmallorylong: No *I* hate Planned Paren...\n",
       "23      Neutral  RT @thekevinryder: #GOPDebate (Vine by @dabull...\n",
       "24     Negative  RT @MrPooni: Fox News trying to convince us yo...\n",
       "25     Negative  #GOPDebate rankings: worst to be performance -...\n",
       "26     Negative  RT @TheBaxterBean: Scott Walker's Abortion Ban...\n",
       "27     Negative  RT @feministabulous: It's not a competition, b...\n",
       "28     Negative  RT @mch7576: RT ÛÏ@TeaTraitors: #GOPDebate wa...\n",
       "29     Negative  RT @HillaryClinton: Watch the #GOPdebate? Bet ...\n",
       "...         ...                                                ...\n",
       "13841  Negative  RT @EthanObama: Did Trump just admit he gives ...\n",
       "13842   Neutral  Trump and most of the World! Just look at the ...\n",
       "13843  Negative  @JessicaValenti they all need coif reform. #GO...\n",
       "13844  Negative  Wow, Rubio's ear-to-head ratio is all messed u...\n",
       "13845  Negative  RT @the818: Let's play \"how fast can we distan...\n",
       "13846   Neutral  Me watching the GOP Debates. #gopdebates https...\n",
       "13847  Positive  RT @mjtbaum: GOD is making an appearance at th...\n",
       "13848  Negative  RT @PuestoLoco: .@NewDay Cancel the primaries....\n",
       "13849  Positive  RT @kaylasmith4791: Really enjoyed everything ...\n",
       "13850  Negative  RT @RWSurferGirl: The candidates don't have to...\n",
       "13851  Negative  #GOPDebate in a nutshell via Vote True Blue 20...\n",
       "13852  Negative  RT @erinmallorylong: No *I* hate Planned Paren...\n",
       "13853  Negative  RT @BrendanKKirby: If @JohnKasich hasn't wrapp...\n",
       "13854  Negative  RT @monaeltahawy: I'll tell you the one good t...\n",
       "13855  Negative  while pro-life nonsense @ the #GOPDebate was n...\n",
       "13856  Negative  GOP respects life....just not black ones. #GOP...\n",
       "13857  Negative  This is why I don't watch Fox News, they're al...\n",
       "13858  Positive  @marcorubio came out of the gate like a true l...\n",
       "13859  Positive  Best line of #GOPDebate was \"Immigration witho...\n",
       "13860  Negative  People who say they are #prolife are usually a...\n",
       "13861  Negative  RT @RWSurferGirl: Why should @realDonaldTrump ...\n",
       "13862  Positive  RT @RWSurferGirl: Trump has got it right, nobo...\n",
       "13863  Negative  So trans soldiers can die for you Huckabee but...\n",
       "13864  Negative  RT @RWSurferGirl: Is it just me or does anyone...\n",
       "13865  Negative  RT @RWSurferGirl: Fox is cherry picking the ca...\n",
       "13866  Negative  RT @cappy_yarbrough: Love to see men who will ...\n",
       "13867  Positive  RT @georgehenryw: Who thought Huckabee exceede...\n",
       "13868  Positive  RT @Lrihendry: #TedCruz As President, I will a...\n",
       "13869  Negative  RT @JRehling: #GOPDebate Donald Trump says tha...\n",
       "13870  Positive  RT @Lrihendry: #TedCruz headed into the Presid...\n",
       "\n",
       "[13871 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data extration\n",
    "\n",
    "df = pd.read_csv(dir_path + '/GOP_REL_ONLY.csv', encoding='ISO-8859-1')\n",
    "to_drop = ['candidate','candidate:confidence','relevant_yn','relevant_yn:confidence',\n",
    "'sentiment:confidence','subject_matter','subject_matter:confidence','candidate_gold',\n",
    "'name','relevant_yn_gold','retweet_count','sentiment_gold','subject_matter_gold',\n",
    "'tweet_coord','tweet_created','tweet_id','tweet_location','user_timezone' ]\n",
    "df.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13871"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()\n",
    "\n",
    "pos = twitter_samples.strings('positive_tweets.json')\n",
    "neg = twitter_samples.strings('negative_tweets.json')\n",
    "pos_df = pd.DataFrame(data=[tweet for tweet in pos], columns=['text'])\n",
    "pos_df['sentiment'] = 'Positive'\n",
    "neg_df = pd.DataFrame(data=[tweet for tweet in neg], columns=['text'])\n",
    "neg_df['sentiment'] = 'Negative'\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23871\n"
     ]
    }
   ],
   "source": [
    "frames = [pos_df, neg_df, df]\n",
    "df = pd.concat(frames)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to pickle file\n",
    "def save_pickle(classifier, name):\n",
    "    save_classifier = open(name + \".pickle\", 'wb')\n",
    "    pickle.dump(classifier, save_classifier)\n",
    "    save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23871"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'RT', ':', ',', '’',\n",
    "                                                   'i\\'m', '…', '...', 'a', 'b', 'c'\n",
    "                                                  'd','g','j','m','p','s','v','y',\n",
    "                                                  'e','h','k','n','q','t','w','z',\n",
    "                                                  'f','i','l','o','r','u','x', 'http', 'https'] \n",
    "\n",
    "def normalizer(tweet):\n",
    "    only_letters = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z\\:\\-\\)\\(])|(\\w+:\\/\\/\\S+)\", \" \", tweet) \n",
    "    tokens = TweetTokenizer().tokenize(only_letters)\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stop, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blahb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)</td>\n",
       "      <td>[followfriday, inte, paris, top, engaged, member, community, week, :)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!</td>\n",
       "      <td>[hey, james, odd, please, call, contact, centre, 02392441234, able, assist, :), many, thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!</td>\n",
       "      <td>[listen, last, night, :), bleed, amazing, track, scotland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>[congrats, :)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days</td>\n",
       "      <td>[yeaaaah, yippppy, accnt, verified, rqst, succeed, got, blue, tick, mark, fb, profile, :), 15, day]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text  \\\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)                  \n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!   \n",
       "2  @DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                      \n",
       "3  @97sides CONGRATS :)                                                                                                             \n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                       \n",
       "\n",
       "                                                                                      normalized_tweet  \n",
       "0  [followfriday, inte, paris, top, engaged, member, community, week, :)]                               \n",
       "1  [hey, james, odd, please, call, contact, centre, 02392441234, able, assist, :), many, thanks]        \n",
       "2  [listen, last, night, :), bleed, amazing, track, scotland]                                           \n",
       "3  [congrats, :)]                                                                                       \n",
       "4  [yeaaaah, yippppy, accnt, verified, rqst, succeed, got, blue, tick, mark, fb, profile, :), 15, day]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1) # Setting this so we can see the full content of cells\n",
    "tweets = df[df.sentiment != 'Neutral']\n",
    "tweets['normalized_tweet'] = tweets.text.apply(normalizer)\n",
    "tweets[['text','normalized_tweet']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blahb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[followfriday inte, inte paris, paris top, top engaged, engaged member, member community, community week, week :), followfriday inte paris, inte paris top, paris top engaged, top engaged member, engaged member community, member community week, community week :)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hey james, james odd, odd please, please call, call contact, contact centre, centre 02392441234, 02392441234 able, able assist, assist :), :) many, many thanks, hey james odd, james odd please, odd please call, please call contact, call contact centre, contact centre 02392441234, centre 02392441234 able, 02392441234 able assist, able assist :), assist :) many, :) many thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[listen last, last night, night :), :) bleed, bleed amazing, amazing track, track scotland, listen last night, last night :), night :) bleed, :) bleed amazing, bleed amazing track, amazing track scotland]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[congrats :)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[yeaaaah yippppy, yippppy accnt, accnt verified, verified rqst, rqst succeed, succeed got, got blue, blue tick, tick mark, mark fb, fb profile, profile :), :) 15, 15 day, yeaaaah yippppy accnt, yippppy accnt verified, accnt verified rqst, verified rqst succeed, rqst succeed got, succeed got blue, got blue tick, blue tick mark, tick mark fb, mark fb profile, fb profile :), profile :) 15, :) 15 day]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                              grams\n",
       "0  [followfriday inte, inte paris, paris top, top engaged, engaged member, member community, community week, week :), followfriday inte paris, inte paris top, paris top engaged, top engaged member, engaged member community, member community week, community week :)]                                                                                                                                          \n",
       "1  [hey james, james odd, odd please, please call, call contact, contact centre, centre 02392441234, 02392441234 able, able assist, assist :), :) many, many thanks, hey james odd, james odd please, odd please call, please call contact, call contact centre, contact centre 02392441234, centre 02392441234 able, 02392441234 able assist, able assist :), assist :) many, :) many thanks]                     \n",
       "2  [listen last, last night, night :), :) bleed, bleed amazing, amazing track, track scotland, listen last night, last night :), night :) bleed, :) bleed amazing, bleed amazing track, amazing track scotland]                                                                                                                                                                                                    \n",
       "3  [congrats :)]                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4  [yeaaaah yippppy, yippppy accnt, accnt verified, verified rqst, rqst succeed, succeed got, got blue, blue tick, tick mark, mark fb, fb profile, profile :), :) 15, 15 day, yeaaaah yippppy accnt, yippppy accnt verified, accnt verified rqst, verified rqst succeed, rqst succeed got, succeed got blue, got blue tick, blue tick mark, tick mark fb, mark fb profile, fb profile :), profile :) 15, :) 15 day]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "def ngrams(input_list):\n",
    "    #onegrams = input_list\n",
    "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
    "    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
    "    return bigrams+trigrams\n",
    "tweets['grams'] = tweets.normalized_tweet.apply(ngrams)\n",
    "tweets[['grams']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def count_words(input):\n",
    "    cnt = collections.Counter()\n",
    "    for row in input:\n",
    "        for word in row:\n",
    "            cnt[word] += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gopdebate gopdebates', 231),\n",
       " ('last night', 175),\n",
       " ('lt 3', 135),\n",
       " ('fox news', 120),\n",
       " ('follow back', 84),\n",
       " ('back :)', 82),\n",
       " ('looking forward', 80),\n",
       " ('rubio gopdebate', 75),\n",
       " ('rating gopdebate', 74),\n",
       " ('cruz trump', 74),\n",
       " ('get rid', 73),\n",
       " ('job get', 73),\n",
       " ('thanks :)', 73),\n",
       " ('expose set', 72),\n",
       " ('bush rubio gopdebate', 72),\n",
       " ('rid bush rubio', 72),\n",
       " ('bush rubio', 72),\n",
       " ('set job get', 72),\n",
       " ('together expose set', 72),\n",
       " ('trump need band', 72)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[(tweets.sentiment == 'Positive')][['grams']].apply(count_words)['grams'].most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gopdebate gopdebates', 1051),\n",
       " ('fox news', 636),\n",
       " ('last night', 419),\n",
       " ('jeb bush', 290),\n",
       " ('chris wallace', 266),\n",
       " ('donald trump', 245),\n",
       " ('debate gopdebate', 226),\n",
       " ('fair amp', 204),\n",
       " ('amp balanced', 202),\n",
       " ('fair amp balanced', 202),\n",
       " ('debate gopdebate gopdebates', 168),\n",
       " ('listen gopdebate', 165),\n",
       " ('bush reminds', 163),\n",
       " ('music hear', 162),\n",
       " ('listen gopdebate gopdebates', 162),\n",
       " ('reminds elevator', 162),\n",
       " ('hear listen gopdebate', 162),\n",
       " ('jeb bush reminds', 162),\n",
       " ('music hear listen', 162),\n",
       " ('hear listen', 162)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[(tweets.sentiment == 'Negative')][['grams']].apply(count_words)['grams'].most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Feature Extraction, build a dictionary of feature indices\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Learn the vocabulary dictionary and return term-document matrix.\n",
    "# Fitting finds the internal parameters of the model\n",
    "# Transform applies those parameters to data\n",
    "vectorized_data = count_vectorizer.fit_transform(tweets.text)\n",
    "\n",
    "# Stack horizontally, remove non-zero feature vectors\n",
    "indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment2target(sentiment):\n",
    "    return {\n",
    "        'Negative': 0,\n",
    "        'Positive' : 1\n",
    "    }[sentiment]\n",
    "\n",
    "# Convert Classification into integer for speed and space efficiency reasons\n",
    "targets = tweets.sentiment.apply(sentiment2target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.25, random_state=0)\n",
    "data_train = data_train[:,1:]\n",
    "data_test = data_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(gamma=0.01, C=100., probability=True, class_weight='balanced', kernel='linear')\n",
    "clf_output = clf.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999228246189466"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(data_test, targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(clf, \"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "ddd = MultinomialNB().fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779471348639784"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.score(data_test, targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999228246189466"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(data_test)\n",
    "\n",
    "np.mean(predicted == targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.83      0.87      0.85      3395\n",
      "   Positive       0.73      0.66      0.70      1788\n",
      "\n",
      "avg / total       0.80      0.80      0.80      5183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blahb\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 20729\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(targets_test, predicted, target_names=tweets.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative    13493\n",
      "Positive    7236 \n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts = tweets.sentiment.value_counts()\n",
    "num_of_tweets = tweets.text.count()\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "       ...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vec_clf = Pipeline([('vectorizer', count_vectorizer), ('svm', clf)])\n",
    "vec_clf.fit(tweets.text, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = vec_clf.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876501519610208"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(vec_clf,'vec_clf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
